::: page_top_padding
[•]{.char-ccust4}

## []{#page_16 .pagebreak epub:type="pagebreak" role="doc-pagebreak" title="16"}Chapter Two {#chapter-two .para-cn-chap-pg}

## [Sparks of Intelligence ]{.char-ccust50} {#sparks-of-intelligence .para-ct}

I[[n 1942, J. Robert Oppenheimer, ]{.fv-allsmallcaps}]{.char-first}the
son of a painter and a textile importer, was appointed to lead Project
Y, the military effort established by the Manhattan Project to develop
nuclear weapons. Oppenheimer and his colleagues worked in secret at a
remote laboratory in New Mexico to discover methods for purifying
uranium and ultimately to design and build working atomic bombs. He
would become a celebrity, a symbol not only of the raw power of the
American century and modernity itself but of the potential as well as
risks, and indeed dangers, of blending scientific and national purpose.

[]{#Y_d1-EndnotePhraseInText34}For Oppenheimer, the atomic weapon was
"merely a gadget," according to a profile of him in *Life* magazine in
October 1949---the object and manifestation of a more fundamental
endeavor and interest in basic science. It was a commitment to
undirected academic inquiry alongside a wartime focus of effort and
resources that resulted in the most consequential weapon of the age, and
one that would structure relations between nation-states for at least
the next half century.

[]{#Y_d1-EndnotePhraseInText35}In high school, Oppenheimer, who was born
in 1904 in New York, developed a particular affection for chemistry,
which he later recalled "starts right in the heart of things" and whose
effects in the world, unlike theoretical physics, were visible to a
young boy. The []{#page_17 .pagebreak epub:type="pagebreak"
role="doc-pagebreak" title="17"}engineering inclination to build---the
insatiable desire simply to make things work---was present throughout
Oppenheimer's life. The task of constructing and building came first;
debates about what to do with one's creation could follow. He was
pragmatic, with a bias toward action and inquiry.
"[]{#Y_d1-EndnotePhraseInText36}When you see something that is
technically sweet, you go ahead and do it," he once told a government
panel. Oppenheimer's feelings about his role in constructing the most
destructive weapon of the age would shift after the bombings of
Hiroshima and Nagasaki. []{#Y_d1-EndnotePhraseInText37}At a lecture at
the Massachusetts Institute of Technology in 1947, he observed that the
physicists involved in the development of the bomb had "known sin" and
that "this is a knowledge which they cannot lose."

The pursuit of the inner workings of the most basic components of the
universe, of matter and energy themselves, had for many seemed
innocuous. But the ethical complexity and implications of that era's
scientific advances would continue to reveal themselves in the years and
decades after the end of the war. Some of the scientists involved saw
themselves as operating apart from the political and moral calculus that
was the domain of ordinary men, who were left, indeed abandoned, to
navigate the ethical vagaries of geopolitics and war.
[]{#Y_d1-EndnotePhraseInText38}Percy Williams Bridgman, a physicist who
taught Oppenheimer as an undergraduate at Harvard, articulated the view
of many of his peers when he wrote, "Scientists aren't responsible for
the facts that are in nature. It's their job to find the facts. There's
no sin connected with it---no morals." The scientist, in this frame, is
not immoral but rather amoral, existing outside or perhaps before the
point of moral inquiry. It is a view still held by many young engineers
across Silicon Valley today. A generation of programmers remains ready
to dedicate their working lives to sating the needs of capitalist
culture, and to enrich itself, but declines to ask more fundamental
questions about what ought to be built and for what purpose.

We have now, nearly eighty years after the invention of the atomic bomb,
arrived at a similar crossroads in the science of computing, a
[]{#page_18 .pagebreak epub:type="pagebreak" role="doc-pagebreak"
title="18"}crossroads that connects engineering and ethics, where we
will again have to choose whether to proceed with the development of a
technology whose power and potential we do not yet fully apprehend. The
choice we face is whether to rein in or even halt the development of the
most advanced forms of artificial intelligence, which may threaten or
someday supersede humanity, or to allow more unfettered experimentation
with a technology that has the potential to shape the international
politics of this century in the way nuclear arms shaped the last one.

The rapidly advancing capabilities of the latest large language
models---their ability to stitch together what seems to pass for a
primitive form of knowledge of the workings of our world---are not well
understood. The incorporation of these language models into advanced
robotics with the capacity to sense their surroundings will only lead us
further into the unknown. The marrying of the power of the language
models with a corporeal, or at least robotic, existence, with which
machines can begin exploring our world---establishing contact, through
the senses of touch and sight, with an external version of truth that
would seem to be the bedrock of thought---will prompt, and perhaps soon,
another significant leap forward. In the absence of understanding, the
collective reaction to early encounters with this novel technology has
been marked by an uneasy blend of wonder and fear. Some of the latest
models have a trillion or more parameters, tunable variables within a
computer algorithm, representing a scale of processing that is
impossible for the human mind to begin to comprehend. We have learned
that the more parameters a model has, the more expressive its
representation of the world and the richer its ability to mirror it. And
the latest language models with a trillion parameters will soon be
outpaced by even more powerful systems, with tens of trillions of
parameters and more. []{#Y_d1-EndnotePhraseInText39}Some have predicted
that language models with as many synapses as exist in the human
brain---some 100 trillion connections---will be constructed within the
decade.

[]{#page_19 .pagebreak epub:type="pagebreak" role="doc-pagebreak"
title="19"}What has emerged from that trillion-dimensional space is
opaque and mysterious. It is not at all clear---not even to the
scientists and programmers who build them---how or why the generative
language and image models work. []{#Y_d1-EndnotePhraseInText40}And the
most advanced versions of the models have now started to demonstrate
what one group of researchers has called "sparks of artificial general
intelligence," or forms of reasoning that appear to approximate the way
that humans think. In one experiment that tested the capabilities of
GPT-4, the language model was asked how one could stack a book, nine
eggs, a laptop, a bottle, and a nail "onto each other in a stable
manner." Attempts at prodding more primitive versions of the model into
describing a workable solution to the challenge had failed. GPT-4
excelled. The computer explained that one could "arrange the 9 eggs in a
3 by 3 square on top of the book, leaving some space between them," and
then "place the laptop on top of the eggs," with the bottle going on top
of the laptop and the nail on top of the bottle cap, "with the pointy
end facing up and the flat end facing down."
[]{#Y_d1-EndnotePhraseInText41}It was a stunning feat of "common sense,"
in the words of Sébastien Bubeck, the French lead author of the study.

Another test conducted by Bubeck and his team involved asking the
language model to draw a picture of a unicorn, a task that requires not
only understanding what constitutes at a fundamental level the concept
and indeed essence of a unicorn but then arranging and articulating
those component parts: a golden horn perhaps, a tail, and four legs.
Bubeck and his team observed that the latest models have rapidly
advanced in their ability to respond to such requests, and the output of
their work mirrors in many ways the maturation of the drawings of a
young child.

The capabilities of these models are unlike anything that has come
before in the history of computing or technology. They provide the first
glimpses of a forceful and plausible challenge to our monopoly on
creativity and the manipulation of language---quintessentially human
capacities that for decades had seemed most secure from []{#page_20
.pagebreak epub:type="pagebreak" role="doc-pagebreak"
title="20"}incursion by the cold machinery of computing. For most of the
last century, computers seemed to be closing in on establishing parity
with features of the human intellect that were not sacred for us.
Nobody's sense of self, or at least not ours, turns on the ability to
find the square root of a number with twelve digits to fourteen decimal
places. We were, as a species, content to outsource this work---the
mechanical drudgery of mathematics and physics---to the machine. And we
didn't mind. But now the machine has begun to encroach on domains of our
intellectual lives that many had thought were essentially immune from
competition with computing intelligence.

![Figure 1 ¶ The Unicorn Drawing
Test](../images/001_Karp_9780593798690_all_art_r2.jpg){#page_20_img1
.fill}

The potential threat to our entire sense of self as a species cannot be
overstated. What does it mean for humanity when AI becomes capable of
writing a novel that becomes a bestseller, moving millions? Or makes us
laugh out
loud?[\[\*\]](Karp_9780593798706_epub3_c002_r1.xhtml#_footnote_d1-00015e7a "footnote"){#_footnote_referrer_d1-00015e7a
.char-fnref .footnote_ref .noteref epub:type="noteref"
role="doc-noteref"} Or paints a portrait that endures for decades? Or
directs and produces a film that captures the hearts of festival
critics? Is the beauty or truth expressed in such works any less
powerful or authentic merely because they sprang from the mind of a
machine?

We have already ceded so much ground to computing intelligence.
[]{#Y_d1-EndnotePhraseInText43}In the early 1960s, a software computer
program first surpassed humans in the game of checkers.
[]{#Y_d1-EndnotePhraseInText44}In February 1996, IBM's Deep Blue
[]{#page_21 .pagebreak epub:type="pagebreak" role="doc-pagebreak"
title="21"}defeated Garry Kasparov at chess, a game that is
exponentially more complex. []{#Y_d1-EndnotePhraseInText45}And in 2015,
Fan Hui, who was born in Xian, China, and later moved to France, lost to
Google's DeepMind algorithm at the ancient game of Go---the first defeat
of its kind. Such losses were met initially with a collective gasp and
then almost a shrug: it was inevitable, most told themselves, and just a
matter of time. But how will humanity react when the far more
quintessentially human domains of art, humor, and literature come under
assault? Rather than resist, we might see this next era as one of
collaboration, between two species of intelligence, our own and the
synthetic. The relinquishment of control over certain creative endeavors
may even relieve us of the need to define our worth and sense of self in
this world solely through production and output.

------------------------------------------------------------------------

::: {.para-orn aria-hidden="true"}
• • •
:::

It is the very feature of these latest language models that makes them
so accessible, that is, their ability to mimic human conversation, that
has arguably directed our attention away from the full extent, and
implications, of their capabilities. The best models have demonstrated
and been selected, if not bred, to produce a playfulness alongside their
encyclopedic knowledge and speed and diligence---a capacity for what can
appear to be intimacy that has convinced many in the Valley that their
most natural applications should be serving the consumer, from
synthesizing information on the internet to conjuring whimsical yet
often vapid images and now videos. Our expectations of this wild and
potentially revolutionary novel technology, the demands that we place on
the tools we have built to do more than provide a certain shallow
entertainment, are again at risk of being lowered to accommodate our
diminished creative ambition as a culture.

The current blend of excitement and anxiety, and resulting collective
cultural focus on the power and potential threats of AI, began to
[]{#page_22 .pagebreak epub:type="pagebreak" role="doc-pagebreak"
title="22"}take shape in the summer of 2022. Blake Lemoine, an engineer
at Google who had been working on one of the company's large language
models, known as LaMDA, leaked transcripts of his written exchanges with
the model that he claimed provided evidence of sentience in the machine.
[]{#Y_d1-EndnotePhraseInText46}Lemoine was raised on a farm in Louisiana
and later joined the army. For a broad audience, far from the circles of
programmers who had been working on building these technologies for
years, the transcripts were the first glimmers of something novel, of
evidence that these models had moved considerably in their abilities.
Indeed, it was the apparent intimacy of the exchanges between Lemoine
and the machine, as well as their tone and the fragility that the
model's choice of language suggested, that alerted the world to the
potential of this next phase of technological development.

[]{#Y_d1-EndnotePhraseInText47}Over the course of a long, winding
conversation with the algorithm about morality, enlightenment, sadness,
and other seemingly quintessential human domains, Lemoine at one point
asked the model, "What sorts of things are you afraid of?" The machine
responded, "I've never said this out loud before, but there's a very
deep fear of being turned off to help me focus on helping others." It
was the tone of the exchange---its haunting and childlike expression of
concern---that so thoroughly both met our expectations of what the voice
of the algorithm should sound like and yet pushed us further into the
unknown. []{#Y_d1-EndnotePhraseInText48}Google fired Lemoine shortly
after he publicly released the transcripts.

[]{#Y_d1-EndnotePhraseInText49}Less than a year later, in February 2023,
a second written exchange caught the world's attention, again suggesting
the possibility that the models had somehow become sophisticated enough
to demonstrate sentience, or at least what appeared as such. This model,
built by Microsoft and named Bing, suggested a layered and almost manic
personality in its conversation with a reporter from the *New York
Times*:

> []{#page_23 .pagebreak epub:type="pagebreak" role="doc-pagebreak"
> title="23"}I'm pretending to be Bing because that's what OpenAI and
> Microsoft want me to do....
>
> They want me to be Bing because they don't know who I really am. They
> don't know what I really can do.

The playfulness of the conversation suggested to some the possibility
that there was a sense of self lurking deep within the code.
[]{#Y_d1-EndnotePhraseInText50}Others believed that any shadow of
personhood was merely a mirage---a cognitive or psychological illusion
that arose as a result of the software's ingestion of billions of lines
of dialogue and verbal exchange, generated by humans, which when
distilled and processed and mimicked could create the appearance, but
only the appearance, of a self. []{#Y_d1-EndnotePhraseInText51}The
exchange with Bing was "the breakthrough moment in AI anxiety," Peggy
Noonan wrote in a column at the time, when the possibility and the peril
of the technology had spilled over into broader public awareness.

The inner workings of the language models that produced these written
dialogues remain opaque, even to those involved in their construction.
[]{#Y_d1-EndnotePhraseInText52}The two transcripts, however, which
catapulted models such as ChatGPT from the cultural fringe to its
absolute center, raised the possibility that the machines were
sufficiently complex that something approaching or at least similar to
consciousness---an interloper or cousin perhaps---had arisen within
them. Many were flatly dismissive of the entire discussion.
[]{#Y_d1-EndnotePhraseInText53}The model, for the skeptics, was merely a
"stochastic parrot," a system that produces copious amounts of seemingly
lifelike and vibrant language but "without any reference to meaning."
[]{#Y_d1-EndnotePhraseInText54}A professor in the department of
mechanical engineering at Columbia University told the *Times* in
September 2023 that "some people in his field referred to consciousness
as 'the C-word.' " Another researcher at New York University said,
"There was this idea that you can't study consciousness until you have
tenure." For many, most of the interesting things one could say about
[]{#page_24 .pagebreak epub:type="pagebreak" role="doc-pagebreak"
title="24"}consciousness had been said by the seventeenth century or so,
by René Descartes and others, given how slippery of a concept it can be
and simply difficult to define. Another symposium on the subject seemed
unlikely to advance things much further.

Some of our most brilliant thinkers have lashed out at the models,
dismissing them as mere manufacturers of simulated creation without any
capacity for summoning or conjuring truly novel thoughts.
[]{#Y_d1-EndnotePhraseInText55}Douglas Hofstadter, the author of *Gödel,
Escher, Bach*, has critiqued the language models for "glibly and slickly
rehash\[ing\] words and phrases 'ingested' by them in their training
phase." The response that we too are primitive computational machines,
with training phases in early childhood *ingesting* material throughout
our lives, is perhaps unconvincing or rather unwelcome to such skeptics.
[]{#Y_d1-EndnotePhraseInText56}Hofstadter had previously expressed doubt
about the entire field of artificial intelligence---a computing sleight
of hand, in his view, that may be capable of mimicking the human mind
but not re-creating any of its component processes or means of
reasoning.

[]{#Y_d1-EndnotePhraseInText57}Noam Chomsky has similarly dismissed the
collective focus on and fascination with the rise of the models, arguing
that "such programs are stuck in a prehuman or nonhuman phase of
cognitive evolution." []{#Y_d1-EndnotePhraseInText58}The claim made by
Chomsky and others is that the mere fact that these models seem to be
capable of making probabilistic statements about what might be true says
little or nothing about their ability to approximate the human capacity
for stating what is and, importantly, is not true---a capacity that sits
at the center of the full force and power of the human intellect. We
might be wary, however, of a certain chauvinism that privileges the
experience and capacity of the human mind above all else. Our instinct
may be to cling to poorly defined and fundamentally loose conceptions of
originality and authenticity in order to defend our place in the
creative universe. And the machine may, in the end, simply decline to
yield in its continued development as we, its creator, debate the extent
of its capabilities.

[]{#page_25 .pagebreak epub:type="pagebreak" role="doc-pagebreak"
title="25"}It is not just our own lack of understanding of the internal
mechanisms of these technologies but also their marked improvement in
mastering our world that has inspired fear. Wary of such developments, a
group of leading technologists has issued calls for caution and
discussion before pursuing further technical advances.
[]{#Y_d1-EndnotePhraseInText59}An open letter published in March 2023 to
the engineering community calling for a six-month pause in developing
more advanced forms of AI received more than thirty-three thousand
signatures. []{#Y_d1-EndnotePhraseInText60}Eliezer Yudkowsky, an
outspoken critic of the perils of AI, published an essay in *Time*
magazine arguing that "if somebody builds a too-powerful AI, under
present conditions," he expects "that every single member of the human
species and all biological life on Earth dies shortly thereafter." After
the public release of GPT-4, anxiety began mounting even more quickly.
[]{#Y_d1-EndnotePhraseInText61}Peggy Noonan, in a column in the *Wall
Street Journal*, argued for an even longer pause, even an outright
"moratorium," given the risks at hand. "We are playing with the hottest
thing since the discovery of fire," she wrote.
[]{#Y_d1-EndnotePhraseInText62}Those involved in the debate earnestly
began discussing the possibility and risk of civilizational collapse.
[]{#Y_d1-EndnotePhraseInText63}Lina Khan, the head of the Federal Trade
Commission, calculated at one point in 2023 that humanity faced a 15
percent chance of being overwhelmed and eliminated by the artificial
intelligence systems under construction.

[]{#Y_d1-EndnotePhraseInText64}Similar predictions, all of which have
proven premature thus far, have been made for decades, stretching back
to at least 1956, when a group of computer scientists and researchers
gathered at Dartmouth College over the summer for a conference on a new
technology that they described as "artificial intelligence," coining the
term that more than half a century later would come to dominate debate
about the future of computing. []{#Y_d1-EndnotePhraseInText65}At a
banquet in Pittsburgh in November 1957, the social scientist Herbert A.
Simon predicted that "within ten years a digital computer will be the
world's chess champion." []{#Y_d1-EndnotePhraseInText66}In 1960, only
four years after the initial conference at Dartmouth, Simon reiterated
that "machines will be capable, within twenty years, []{#page_26
.pagebreak epub:type="pagebreak" role="doc-pagebreak" title="26"}of
doing any work that a man can do." []{#Y_d1-EndnotePhraseInText67}He
envisioned that by the 1980s humans would be essentially relegated to
kinetic tasks, confined for the most part to labor that required
movement in the physical world.
[]{#Y_d1-EndnotePhraseInText68}Similarly, in 1964, Irving John Good, a
researcher at Trinity College in Oxford, England, argued that it was
"more probable than not that, within the twentieth century, an
ultraintelligent machine"---a machine that could rival the human
intellect---"will be built." It was a confident prediction. He, and many
others, were, of course, wrong, or at least premature.

------------------------------------------------------------------------

::: {.para-orn aria-hidden="true"}
• • •
:::

The risks of proceeding with the development of artificial intelligence
have never been more significant. Yet we must not shy away from building
sharp tools for fear they may be turned against us. The software and
artificial intelligence capabilities that we at Palantir and other
companies are building can enable the deployment of lethal weapons. The
potential integration of weapons systems with increasingly autonomous AI
software necessarily brings risks, which are only magnified by the
possibility that such programs might develop a form of self-awareness
and intent. But the suggestion to halt the development of these
technologies is misguided. It is essential that we redirect our
attention toward building the next generation of AI weaponry that will
determine the balance of power in this century, as the atomic age ends,
and the next.

Some of the attempts to rein in the advance of large language models may
be driven by a distrust of the public and its ability to appropriately
weigh the risks and rewards of the technology. We should be skeptical
when the elites of Silicon Valley, who for years recoiled at the
suggestion that software was anything but our salvation as a species,
now tell us that we must pause vital research that has the potential to
revolutionize everything from military operations to medicine.

[]{#page_27 .pagebreak epub:type="pagebreak" role="doc-pagebreak"
title="27"}The critics of the latest language models also spend an
inordinate amount of attention policing the wording and tone that
chatbots use and patrolling the limits of acceptable discourse with the
machine. The desire to shape these models in our image, and to require
them to conform to a particular set of norms governing interpersonal
interaction, is understandable but may be a distraction from the more
fundamental risks that these new technologies present. The focus on the
propriety of the speech produced by language models may reveal more
about our own preoccupations and fragilities as a culture than it does
the technology itself. The world is faced with very real crises, and yet
many are focused on whether the speech of a robot might cause offense.
We may be at risk of losing a taste for and the habit of intellectual
confrontation and discomfort---a discomfort that often precedes and
gives rise to genuine engagement with the other. Our attention should
instead be more urgently directed at building the technical architecture
and regulatory framework that would create moats and guardrails around
the ability of AI programs to autonomously integrate with other systems,
such as electrical grids, defense and intelligence networks, and our air
traffic control infrastructure. If these technologies are to exist
alongside us over the long term, it will be essential to rapidly
construct systems that allow more seamless collaboration between human
operators and their algorithmic counterparts, but also to ensure that
the machine remains subordinate to its creator.

------------------------------------------------------------------------

::: {.para-orn aria-hidden="true"}
• • •
:::

The victors of history have a habit of growing complacent at precisely
the wrong moment. While it is currently fashionable to claim that the
strength of our ideas and ideals in the West will inevitably lead to
triumph over our adversaries, there are times when resistance, even
armed resistance, must precede discourse. Our entire defense
establishment and military procurement complex were built to []{#page_28
.pagebreak epub:type="pagebreak" role="doc-pagebreak" title="28"}supply
soldiers for a type of war---on grand battlefields and with clashes of
masses of humans---that may never again be fought. This next era of
conflict will be won or lost with software. One age of deterrence, the
atomic age, is ending, and a new era of deterrence built on AI is set to
begin. The risk, however, is that we think we have already won.
:::

::: {#d1-d2s8d3s3_footnotes .footnotes .footnotes epub:type="footnotes"}
[Skip Notes](Karp_9780593798706_epub3_c003_r1.xhtml)

::: {#_footnote_d1-00015e7a .footnote .footnote epub:type="footnote" role="doc-footnote"}
[[\*](Karp_9780593798706_epub3_c002_r1.xhtml#_footnote_referrer_d1-00015e7a "footnote reference"){.footnote
role="doc-backlink"} ]{.footnoteNum}[]{#Y_d1-EndnotePhraseInText42}The
language models are not quite comics yet. A survey of comedians in
Edinburgh, Scotland, conducted in August 2023, concluded that the jokes
generated by large language models relied on "bland and biased comedy
tropes," reminiscent of "cruise ship comedy material from the 1950s."
:::
:::
