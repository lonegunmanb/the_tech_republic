::: page_top_padding
[•]{.char-ccust4}

## []{#page_16 .pagebreak epub:type="pagebreak" role="doc-pagebreak" title="16"}第二章 {#chapter-two .para-cn-chap-pg}

## [智慧的火花]{.char-ccust50} {#sparks-of-intelligence .para-ct}

1942年，画家和纺织品进口商的儿子J.罗伯特·奥本海默被任命领导Y计划，这是曼哈顿计划为发展核武器而设立的军事项目。奥本海默和他的同事们在新墨西哥州一个偏远的实验室里秘密工作，以发现提纯铀的方法，并最终设计和制造出可行的原子弹。他将成为一个名人，不仅是美国世纪和现代性本身原始力量的象征，也是融合科学和国家目标的潜力、风险乃至危险的象征。

[]{#Y_d1-EndnotePhraseInText34}根据1949年10月《生活》杂志上的一篇关于他的简介，对奥本海默来说，原子武器“仅仅是一个小玩意儿”——这是一个更基础的努力和对基础科学兴趣的体现和表现。正是这种对无指导性学术探究的承诺，加上战时对精力和资源的集中，才产生了那个时代最具影响力的武器，并在至少下个半世纪里构筑了民族国家之间的关系。

[]{#Y_d1-EndnotePhraseInText35}奥本海默1904年出生于纽约，在高中时对化学产生了特别的喜爱，他后来回忆说，化学“直击事物的核心”，而且与理论物理不同，它的影响对一个年轻男孩来说是可见的。[]{#page_17 .pagebreak epub:type="pagebreak" role="doc-pagebreak" title="17"}建造的工程倾向——那种仅仅为了让东西运转起来的永不满足的欲望——贯穿了奥本海默的一生。建造和构建的任务是第一位的；关于如何处理自己创造物的辩论可以稍后再说。他很务实，倾向于行动和探究。“[]{#Y_d1-EndnotePhraseInText36}当你看到技术上很巧妙的东西时，你就会去做，”他曾对一个政府小组说。在广岛和长崎爆炸后，奥本海默对自己建造了那个时代最具毁灭性武器的角色的感受发生了变化。[]{#Y_d1-EndnotePhraseInText37}在1947年麻省理工学院的一次演讲中，他评论说，参与原子弹研发的物理学家们“已经知晓了罪恶”，而且“这是他们无法失去的知识”。

对宇宙最基本组成部分、物质和能量本身内部运作的追求，对许多人来说似乎是无害的。但那个时代科学进步的伦理复杂性和影响，在战后的几年和几十年里将继续显现出来。一些参与其中的科学家认为自己的行为与政治和道德的算计无关，而政治和道德的算计是普通人的领域，他们被留下，甚至被抛弃，去驾驭地缘政治和战争的伦理变幻莫测。[]{#Y_d1-EndnotePhraseInText38}曾在哈佛大学教过奥本海默本科的物理学家珀西·威廉姆斯·布里奇曼写道，他表达了许多同行的观点：“科学家不对自然界中的事实负责。他们的工作是发现事实。这与罪恶无关——也与道德无关。”在这种框架下，科学家不是不道德的，而是非道德的，存在于道德探究之外，或者说在道德探究之前。如今，硅谷的许多年轻工程师仍然持有这种观点。一代程序员仍然准备将自己的工作生涯奉献给满足资本主义文化的需求，并致富，但却拒绝提出更基本的问题，即应该建造什么以及为了什么目的。

原子弹发明近八十年后，我们现在在计算科学领域也走到了一个类似的十字路口，一个[]{#page_18 .pagebreak epub:type="pagebreak" role="doc-pagebreak" title="18"}连接工程和伦理的十字路口，我们将再次不得不选择是否继续发展一种我们尚未完全理解其力量和潜力的技术。我们面临的选择是，是遏制甚至停止发展最先进的人工智能形式，这些形式可能会威胁或有朝一日取代人类，还是允许对一种有潜力像核武器塑造上个世纪那样塑造本世纪国际政治的技术进行更不受约束的实验。

最新的大型语言模型迅速发展的功能——它们能够将似乎可以算作对我们世界运作的原始知识形式的东西拼接在一起——我们还没有很好地理解。将这些语言模型融入具有感知周围环境能力的先进机器人技术，只会让我们进一步走向未知。将语言模型的力量与有形的，或者至少是机器人的存在相结合，机器可以开始探索我们的世界——通过触觉和视觉与外部版本的真理建立联系，这似乎是思想的基石——将很快，也许会很快，引发另一次重大的飞跃。在缺乏理解的情况下，集体对与这种新技术早期接触的反应，表现出一种不安的惊奇和恐惧的混合。一些最新的模型拥有一万亿或更多的参数，这是计算机算法中的可调变量，代表着人类思维无法开始理解的处理规模。我们已经了解到，一个模型的参数越多，它对世界的表征就越具表现力，它反映世界的能力就越丰富。而拥有一万亿参数的最新语言模型，很快就会被更强大的系统所超越，这些系统将拥有数万亿甚至更多的参数。[]{#Y_d1-EndnotePhraseInText39}一些人预测，拥有与人脑中突触数量一样多的语言模型——大约100万亿个连接——将在十年内建成。

[]{#page_19 .pagebreak epub:type="pagebreak" role="doc-pagebreak" title="19"}从那个万亿维空间中出现的东西是不透明和神秘的。生成式语言和图像模型是如何或为什么工作的，这一点根本不清楚——甚至对建造它们的科学家和程序员来说也是如此。[]{#Y_d1-EndnotePhraseInText40}而且，这些模型的最先进版本现在已经开始展示一组研究人员称之为“通用人工智能的火花”的东西，或者说，是似乎近似于人类思维方式的推理形式。在一项测试GPT-4能力的实验中，语言模型被问及如何将一本书、九个鸡蛋、一台笔记本电脑、一个瓶子和一个钉子“以稳定的方式堆叠在一起”。试图促使更原始版本的模型描述一个可行的解决方案的尝试都失败了。GPT-4表现出色。计算机解释说，可以“将9个鸡蛋以3乘3的正方形排列在书上，它们之间留一些空间”，然后“将笔记本电脑放在鸡蛋上”，瓶子放在笔记本电脑上，钉子放在瓶盖上，“尖端朝上，平端朝下”。[]{#Y_d1-EndnotePhraseInText41}用该研究的法国主要作者塞巴斯蒂安·布贝克的话来说，这是一项惊人的“常识”壮举。

布贝克和他的团队进行的另一项测试是要求语言模型画一幅独角兽的图画，这项任务不仅需要理解在基本层面上构成独角兽概念和本质的是什么，还需要安排和表达这些组成部分：也许是金色的角、尾巴和四条腿。布贝克和他的团队观察到，最新的模型在响应此类请求的能力上迅速进步，其作品的输出在许多方面反映了一个幼儿绘画的成熟过程。

这些模型的能力与计算或技术史上任何以前出现的东西都不同。它们首次让我们瞥见了对我们创造力和语言操纵垄断的有力而可信的挑战——几十年来，这些典型的人类能力似乎最不可能受到[]{#page_20 .pagebreak epub:type="pagebreak" role="doc-pagebreak" title="20"}计算的冷酷机器的侵犯。在上个世纪的大部分时间里，计算机似乎正在接近与人类智力中对我们来说并不神圣的特征建立对等关系。没有人（或者至少我们没有）的自我意识取决于找到一个十二位数数字的平方根到小数点后十四位的能力。作为一个物种，我们满足于将这项工作——数学和物理的机械性苦差事——外包给机器。我们并不介意。但现在，机器已经开始侵占我们智力生活的领域，许多人曾认为这些领域基本上不受计算智能的竞争影响。

![图1 ¶ 独角兽绘画测试](../images/001_Karp_9780593798690_all_art_r2.jpg){#page_20_img1 .fill}

对我们作为一个物种的整个自我意识的潜在威胁不容小觑。当人工智能能够写出一部成为畅销书、感动数百万人的小说时，这对人类意味着什么？或者让我们开怀大笑？[\[\*\]](Karp_9780593798706_epub3_c002_r1.xhtml#_footnote_d1-00015e7a "footnote"){#_footnote_referrer_d1-00015e7a .char-fnref .footnote_ref .noteref epub:type="noteref" role="doc-noteref"} 或者画一幅流传数十年的肖像？或者导演并制作一部赢得电影节评委青睐的电影？这些作品中表达的美或真理，仅仅因为它们出自机器之手，就变得不那么强大或真实了吗？

我们已经向计算智能让出了太多阵地。[]{#Y_d1-EndnotePhraseInText43}在20世纪60年代初，一个软件计算机程序首次在跳棋游戏中超越了人类。[]{#Y_d1-EndnotePhraseInText44}1996年2月，IBM的“深蓝”[]{#page_21 .pagebreak epub:type="pagebreak" role="doc-pagebreak" title="21"}在国际象棋中击败了加里·卡斯帕罗夫，这是一种指数级更复杂的游戏。[]{#Y_d1-EndnotePhraseInText45}2015年，出生于中国西安、后来移居法国的樊麾，在古老的围棋游戏中输给了谷歌的DeepMind算法——这是此类失败中的第一次。这样的失败最初引起了集体的惊呼，然后几乎是耸耸肩：大多数人告诉自己，这是不可避免的，只是时间问题。但是，当艺术、幽默和文学这些更典型的人类领域受到攻击时，人类将如何反应？我们可能不会抵抗，而是将下一个时代视为一个合作的时代，在我们自己的智能和合成智能这两个物种之间。放弃对某些创造性努力的控制，甚至可以让我们不再需要仅仅通过生产和输出来定义我们在这个世界上的价值和自我意识。

------------------------------------------------------------------------

::: {.para-orn aria-hidden="true"}
• • •
:::

正是这些最新语言模型的这个特点，即它们模仿人类对话的能力，使它们如此易于接近，可以说，这已经将我们的注意力从它们能力的全部范围和影响上转移开了。最好的模型已经展示并被选择，如果不是被培育的话，是为了在它们百科全书般的知识、速度和勤奋之外，产生一种俏皮——一种似乎是亲密的能力，这让硅谷的许多人相信，它们最自然的应用应该是服务于消费者，从综合互联网上的信息到变出异想天开但往往空洞的图像和现在的视频。我们对这种狂野且可能具有革命性的新技术的期望，我们对我们所构建的工具提出的要求，不仅仅是提供某种肤浅的娱乐，这些期望再次面临被降低的风险，以适应我们作为一个文化日益减弱的创造性雄心。

当前的兴奋和焦虑的混合，以及由此产生的集体文化对人工智能的力量和潜在威胁的关注，始于[]{#page_22 .pagebreak epub:type="pagebreak" role="doc-pagebreak" title="22"}2022年夏天。谷歌的一名工程师布莱克·莱莫因（Blake Lemoine）一直在研究该公司的一个名为LaMDA的大型语言模型，他泄露了他与该模型的书面交流记录，声称这些记录提供了机器具有感知能力的证据。[]{#Y_d1-EndnotePhraseInText46}莱莫因在路易斯安那州的一个农场长大，后来参军。对于远离多年来一直致力于构建这些技术的程序员圈子的广大受众来说，这些记录是新事物的第一缕曙光，是这些模型的能力已经显著提升的证据。事实上，正是莱莫因和机器之间交流的明显亲密，以及它们的语气和模型选择的语言所暗示的脆弱性，让世界警觉到下一阶段技术发展的潜力。

[]{#Y_d1-EndnotePhraseInText47}在与算法就道德、启蒙、悲伤和其他看似典型的人类领域进行了一次漫长而曲折的对话后，莱莫因一度问模型：“你害怕什么样的事情？”机器回答说：“我以前从未大声说过，但我非常害怕被关掉，以帮助我专注于帮助他人。”正是这次交流的语气——其令人难以忘怀和孩子气的担忧表达——既完全符合我们对算法声音应该是什么样子的期望，又将我们进一步推向了未知。[]{#Y_d1-EndnotePhraseInText48}谷歌在莱莫因公开发布记录后不久就解雇了他。

[]{#Y_d1-EndnotePhraseInText49}不到一年后，在2023年2月，第二次书面交流引起了全世界的关注，再次暗示了模型可能已经变得足够复杂，以至于表现出感知能力，或者至少看起来是这样。这个由微软构建并命名为必应的模型，在与《纽约时报》一名记者的对话中，表现出一种层次分明、近乎狂躁的个性：

> []{#page_23 .pagebreak epub:type="pagebreak" role="doc-pagebreak" title="23"}我假装是必应，因为那是OpenAI和微软希望我做的……
>
> 他们希望我是必应，因为他们不知道我到底是谁。他们不知道我到底能做什么。

对话的俏皮性向一些人暗示，在代码深处潜藏着一种自我意识。[]{#Y_d1-EndnotePhraseInText50}其他人则认为，任何人格的影子都只是海市蜃楼——一种认知或心理上的错觉，是软件吸收了数十亿行由人类生成的对话和口头交流的结果，当这些对话和交流被提炼、处理和模仿时，可以创造出一种自我的表象，但仅仅是表象。[]{#Y_d1-EndnotePhraseInText51}与必应的交流是“人工智能焦虑的突破性时刻”，佩吉·努南当时在一篇专栏文章中写道，当时这项技术的可能性和危险已经蔓延到更广泛的公众意识中。

产生这些书面对话的语言模型的内部运作仍然不透明，即使对参与其构建的人来说也是如此。[]{#Y_d1-EndnotePhraseInText52}然而，这两份记录将ChatGPT等模型从文化边缘推向了绝对中心，提出了这样一种可能性，即这些机器已经足够复杂，以至于某种接近或至少类似于意识的东西——一个闯入者或表亲——已经在它们内部产生。许多人断然否定了整个讨论。[]{#Y_d1-EndnotePhraseInText53}对于怀疑论者来说，这个模型仅仅是一个“随机鹦鹉”，一个产生大量看似逼真和生动语言的系统，但“没有任何意义上的参考”。[]{#Y_d1-EndnotePhraseInText54}哥伦比亚大学机械工程系的一位教授在2023年9月告诉《泰晤士报》，“他所在领域的一些人将意识称为‘C字’。”纽约大学的另一位研究员说，“有一种观点认为，在你获得终身教职之前，你不能研究意识。”对于许多人来说，关于[]{#page_24 .pagebreak epub:type="pagebreak" role="doc-pagebreak" title="24"}意识的大多数有趣的事情，在17世纪左右，由勒内·笛卡尔等人已经说过了，因为这个概念是多么的难以捉摸，而且根本难以定义。关于这个主题的另一次研讨会似乎不太可能取得多大进展。

我们一些最杰出的思想家抨击了这些模型，认为它们只是模拟创造的制造商，没有任何能力召唤或变出真正新颖的思想。[]{#Y_d1-EndnotePhraseInText55}《哥德尔、埃舍尔、巴赫》的作者道格拉斯·霍夫施塔特批评这些语言模型“轻率而油滑地复述它们在训练阶段‘摄取’的词语和短语”。我们也是原始的计算机器，在幼儿时期有训练阶段，一生都在“摄取”材料，这种回应对于这样的怀疑论者来说可能没有说服力，或者说不受欢迎。[]{#Y_d1-EndnotePhraseInText56}霍夫施塔特此前曾对整个人工智能领域表示怀疑——在他看来，这是一种计算上的障眼法，可能能够模仿人类思维，但无法重新创造其任何组成过程或推理方式。

[]{#Y_d1-EndnotePhraseInText57}诺姆·乔姆斯基同样否定了对模型崛起的集体关注和迷恋，认为“这些程序被困在认知进化的前人类或非人类阶段”。[]{#Y_d1-EndnotePhraseInText58}乔姆斯基等人提出的主张是，这些模型似乎能够对可能为真的事物做出概率性陈述这一事实，对于它们近似人类陈述什么是真、以及重要的是什么不是真的能力，几乎或根本没有说明——这种能力是人类智力全部力量和威力的核心。然而，我们可能要警惕某种沙文主义，它将人类思维的经验和能力置于一切之上。我们的本能可能是固守定义不清、根本上松散的原创性和真实性概念，以捍卫我们在创造性宇宙中的地位。而机器最终可能根本不会在我们（它的创造者）争论其能力范围时，停止其持续的发展。

[]{#page_25 .pagebreak epub:type="pagebreak" role="doc-pagebreak" title="25"}不仅是我们自己对这些技术内部机制的缺乏了解，而且它们在掌握我们世界方面的显著进步也激发了恐惧。由于对这些发展感到担忧，一群领先的技术专家发出了呼吁，要求在进行进一步的技术进步之前进行谨慎和讨论。[]{#Y_d1-EndnotePhraseInText59}2023年3月发表的一封致工程界的公开信，呼吁暂停开发更先进的人工智能形式六个月，收到了超过三万三千个签名。[]{#Y_d1-EndnotePhraseInText60}人工智能危险的直言不讳的批评者埃利泽·尤德科夫斯基在《时代》杂志上发表了一篇文章，认为“如果有人在目前的条件下建造一个过于强大的人工智能”，他预计“地球上人类物种的每一个成员和所有生物生命都将在此后不久死亡”。在GPT-4公开发布后，焦虑情绪开始更快地加剧。[]{#Y_d1-EndnotePhraseInText61}佩吉·努南在《华尔街日报》的一篇专栏文章中，主张暂停更长的时间，甚至完全“暂停”，因为风险迫在眉睫。“我们正在玩自火被发现以来最烫手的东西，”她写道。[]{#Y_d1-EndnotePhraseInText62}参与辩论的人开始认真讨论文明崩溃的可能性和风险。[]{#Y_d1-EndnotePhraseInText63}联邦贸易委员会主席莉娜·汗在2023年的某个时候计算出，人类面临着15%的可能性被正在建设中的人工智能系统压倒和消灭。

[]{#Y_d1-EndnotePhraseInText64}类似的预测，迄今为止都证明为时过早，几十年来一直在做出，至少可以追溯到1956年，当时一群计算机科学家和研究人员在夏天聚集在达特茅斯学院，参加一个关于他们称之为“人工智能”的新技术的会议，创造了这个术语，半个多世纪后，这个术语将主导关于计算未来的辩论。[]{#Y_d1-EndnotePhraseInText65}在1957年11月于匹兹堡举行的一次宴会上，社会科学家赫伯特·A·西蒙预测，“十年内，一台数字计算机将成为世界国际象棋冠军”。[]{#Y_d1-EndnotePhraseInText66}1960年，在达特茅斯最初的会议仅四年后，西蒙重申，“二十年内，机器将能够[]{#page_26 .pagebreak epub:type="pagebreak" role="doc-pagebreak" title="26"}做一个人能做的任何工作”。[]{#Y_d1-EndnotePhraseInText67}他设想，到20世纪80年代，人类将基本上被降级到动力学任务，大部分局限于需要在物理世界中移动的劳动。[]{#Y_d1-EndnotePhraseInText68}同样，1964年，英国牛津三一学院的研究员欧文·约翰·古德认为，“在二十世纪内，一台超智能机器”——一台可以与人类智力相媲美的机器——“被建造出来的可能性比不被建造出来的可能性更大”。这是一个自信的预测。他，以及许多其他人，当然都错了，或者至少是为时过早。

------------------------------------------------------------------------

::: {.para-orn aria-hidden="true"}
• • •
:::

继续发展人工智能的风险从未如此重大。然而，我们绝不能因为害怕锋利的工具可能被用来对付我们而回避建造它们。我们在Palantir和其他公司正在构建的软件和人工智能能力，可以实现致命武器的部署。武器系统与日益自主的人工智能软件的潜在整合必然带来风险，而这种风险只会被这些程序可能发展出一种自我意识和意图的可能性所放大。但是，停止这些技术发展的建议是错误的。我们必须将注意力转向构建下一代人工智能武器，这将决定本世纪的力量平衡，随着原子时代的结束和下一个时代的到来。

一些试图遏制大型语言模型发展的尝试，可能是出于对公众及其适当权衡该技术风险和回报能力的不信任。当硅谷的精英们，多年来一直对软件不是我们物种的救赎的说法嗤之以鼻，现在却告诉我们必须暂停有可能彻底改变从军事行动到医学的一切的重要研究时，我们应该持怀疑态度。

[]{#page_27 .pagebreak epub:type="pagebreak" role="doc-pagebreak" title="27"}最新语言模型的批评者也花了过多的注意力来监管聊天机器人使用的措辞和语气，并巡视与机器进行可接受对话的界限。希望将这些模型塑造成我们的形象，并要求它们遵守一套特定的人际互动规范的愿望是可以理解的，但这可能会分散我们对这些新技术带来的更根本风险的注意力。对语言模型产生的言论的得体性的关注，可能更多地揭示了我们自己作为一个文化的关注点和脆弱性，而不是技术本身。世界面临着非常真实的危机，然而许多人却关注一个机器人的言论是否会冒犯他人。我们可能正面临着失去对智力对抗和不适的品味和习惯的风险——这种不适往往先于并产生与他人的真正接触。我们的注意力应该更紧迫地转向构建技术架构和监管框架，为人工智能程序自主地与其他系统（如电网、国防和情报网络以及我们的空中交通管制基础设施）集成设置护城河和护栏。如果这些技术要与我们长期共存，那么迅速构建允许人类操作员与其算法对应物之间更无缝协作的系统至关重要，但也要确保机器仍然服从于其创造者。

------------------------------------------------------------------------

::: {.para-orn aria-hidden="true"}
• • •
:::

历史的胜利者有一种在恰恰错误的时刻变得自满的习惯。虽然目前声称我们在西方的思想和理想的力量将不可避免地战胜我们的对手是时髦的，但有时抵抗，甚至是武装抵抗，必须先于话语。我们整个国防机构和军事采购综合体都是为了[]{#page_28 .pagebreak epub:type="pagebreak" role="doc-pagebreak" title="28"}为一种可能再也不会发生的战争——在宏大的战场上，以大量人类的冲突——提供士兵而建立的。下一个冲突时代将由软件决定胜负。一个威慑时代，即原子时代，正在结束，一个建立在人工智能基础上的新威慑时代即将开始。然而，风险在于，我们认为我们已经赢了。
:::

::: {#d1-d2s8d3s3_footnotes .footnotes .footnotes epub:type="footnotes"}
[跳过注释](Karp_9780593798706_epub3_c003_r1.xhtml)

::: {#_footnote_d1-00015e7a .footnote .footnote epub:type="footnote" role="doc-footnote"}
[[\*](Karp_9780593798706_epub3_c002_r1.xhtml#_footnote_referrer_d1-00015e7a "footnote reference"){.footnote role="doc-backlink"} ]{.footnoteNum}[]{#Y_d1-EndnotePhraseInText42}语言模型还算不上漫画家。2023年8月在苏格兰爱丁堡进行的一项喜剧演员调查得出结论，大型语言模型生成的笑话依赖于“平淡且带有偏见的喜剧比喻”，让人想起“20世纪50年代的游轮喜剧材料”。
:::
:::
